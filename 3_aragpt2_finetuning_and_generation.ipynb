# =============================================
# 3. Fine-tuning AraGPT2 & Text Generation
# =============================================

!pip install -q transformers datasets accelerate torch

from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from datasets import Dataset
import torch
import pandas as pd

# -----------------------------
# Load raw texts for generation task
# -----------------------------
df_raw = pd.read_csv("arabic_politics_raw.csv")  # use original non-cleaned texts
texts = df_raw['text'].dropna().tolist()

# Create Hugging Face dataset
hf_dataset = Dataset.from_dict({"text": texts})

# -----------------------------
# Load AraGPT2 base model
# -----------------------------
model_name = "aubmindlab/aragpt2-base"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Required for training
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

# -----------------------------
# Tokenization
# -----------------------------
max_length = 512

def tokenize_batch(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=max_length
    )

tokenized_dataset = hf_dataset.map(tokenize_batch, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(["text"])
tokenized_dataset.set_format("torch", columns=["input_ids", "attention_mask"])

# Split (optional, here we use full for fine-tuning)
train_dataset = tokenized_dataset

# -----------------------------
# Training arguments
# -----------------------------
training_args = TrainingArguments(
    output_dir="./aragpt2-politics-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=4,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    learning_rate=5e-5,
    fp16=True,  # GPU acceleration
    logging_steps=20,
    save_steps=200,
    save_total_limit=2,
    report_to=[],
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

print("Starting fine-tuning...")
trainer.train()

# Save fine-tuned model
model.save_pretrained("./aragpt2-politics-finetuned")
tokenizer.save_pretrained("./aragpt2-politics-finetuned")
print("Fine-tuning completed and model saved!")

# -----------------------------
# Text Generation Demo
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

prompts = [
    "في المغرب، الحكومة الجديدة برئاسة عزيز أخنوش",
    "العلاقات بين المغرب وإسرائيل بعد اتفاقيات أبراهام",
    "انتخابات المغرب 2021 شهدت"
]

for prompt in prompts:
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    output = model.generate(
        **inputs,
        max_length=200,
        temperature=0.9,
        top_p=0.95,
        do_sample=True,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id
    )
    
    generated = tokenizer.decode(output[0], skip_special_tokens=True)
    print(f"\nPrompt: {prompt}")
    print(f"Generated:\n{generated}")
    print("-" * 80)
