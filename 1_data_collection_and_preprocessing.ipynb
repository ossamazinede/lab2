# =============================================
# 1. Data Collection & Preprocessing (Arabic NLP Lab)
# =============================================

# Install required libraries
!pip install -q beautifulsoup4 requests pandas nltk pyarabic arabic-stopwords

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.isri import ISRIStemmer
import pyarabic.araby as araby
from random import uniform, random
nltk.download('stopwords')
nltk.download('punkt')

# -----------------------------
# Step 1: Scraping Arabic political texts
# -----------------------------

def scrape_paragraphs(url_list, max_per_site=80):
    all_texts = []
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
    
    for base_url in url_list:
        try:
            response = requests.get(base_url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            paragraphs = soup.find_all('p')
            count = 0
            for p in paragraphs:
                text = p.get_text(strip=True)
                if 100 < len(text) < 1000:  # reasonable length
                    all_texts.append(text)
                    count += 1
                    if count >= max_per_site:
                        break
        except Exception as e:
            print(f"Error scraping {base_url}: {e}")
            continue
    return all_texts

# Political Arabic news sites
urls = [
    "https://www.aljazeera.net/politics/",
    "https://arabic.cnn.com/politics",
    "https://www.hespress.com/politique/",
    "https://www.bbc.com/arabic/topics/c404v5rn1y3t"  # BBC Arabic politics
]

print("Starting scraping...")
raw_texts = scrape_paragraphs(urls, max_per_site=80)
print(f"Collected {len(raw_texts)} paragraphs.")

# -----------------------------
# Step 2: Manual-like relevance scoring (simulated)
# -----------------------------

high_relevance_keywords = ["حكومة", "انتخابات", "برلمان", "أخنوش", "بايدن", "فلسطين", "إسرائيل", "سياسة", "وزير"]
low_relevance_keywords  = ["مباراة", "كرة", "فن", "موسيقى", "فيلم", "رياضة", "طقس"]

def assign_relevance_score(text):
    score = 5.0
    if any(word in text for word in high_relevance_keywords):
        score += uniform(2.0, 5.0)
    if any(word in text for word in low_relevance_keywords):
        score -= uniform(2.5, 4.5)
    return round(max(0.0, min(10.0, score)), 1)

data = [{"text": txt, "score": assign_relevance_score(txt)} for txt in raw_texts[:250]]  # limit for demo

df = pd.DataFrame(data)
df.to_csv("arabic_politics_raw.csv", index=False)
print("Raw dataset saved (250 samples).")
print(df.head(10))

# -----------------------------
# Step 3: Arabic NLP Preprocessing Pipeline
# -----------------------------

stop_words = set(stopwords.words('arabic'))
# Add more common Arabic stop words if needed
extra_stops = {"في", "على", "من", "إلى", "و", "أن", "لا", "ما"}
stop_words.update(extra_stops)

stemmer = ISRIStemmer()

def preprocess_arabic_text(text):
    # 1. Remove diacritics & normalize
    text = araby.strip_tashkeel(text)
    text = araby.normalize_hamza(text)
    text = araby.normalize_alef(text)
    text = araby.normalize_teh(text)
    
    # 2. Keep only Arabic letters and spaces
    text = re.sub(r'[^\u0600-\u06FF\s]', ' ', text)
    
    # 3. Tokenize
    tokens = araby.tokenize(text)
    
    # 4. Remove stop words & short tokens + stemming
    tokens = [stemmer.stem(t) for t in tokens if t not in stop_words and len(t) > 2]
    
    return " ".join(tokens)

df['clean_text'] = df['text'].apply(preprocess_arabic_text)

# Remove empty cleaned texts
df = df[df['clean_text'].str.len() > 10].reset_index(drop=True)

df[['text', 'clean_text', 'score']].to_csv("arabic_politics_dataset.csv", index=False)
print("Final cleaned dataset saved!")
print(df[['text', 'score']].head())
print(f"Final dataset size: {len(df)} samples")
