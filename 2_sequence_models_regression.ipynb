# =============================================
# 2. RNN / BiRNN / GRU / LSTM for Relevance Score Regression
# =============================================

!pip install -q torch scikit-learn pandas numpy

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# -----------------------------
# Load cleaned dataset
# -----------------------------
df = pd.read_csv("arabic_politics_dataset.csv")
texts = df['clean_text'].values
scores = df['score'].values.astype(np.float32)

# -----------------------------
# TF-IDF Vectorization (better than CountVectorizer for text)
# -----------------------------
vectorizer = TfidfVectorizer(max_features=8000, ngram_range=(1,2))
X = vectorizer.fit_transform(texts).toarray()
y = scores

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

class TextDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)
    
    def __len__(self): return len(self.y)
    def __getitem__(self, idx): return self.X[idx], self.y[idx]

train_ds = TextDataset(X_train, y_train)
test_ds  = TextDataset(X_test, y_test)

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader  = DataLoader(test_ds,  batch_size=32)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# -----------------------------
# Recurrent Model Definition
# -----------------------------
class RecurrentRegressor(nn.Module):
    def __init__(self, input_size, hidden_size=256, num_layers=2, 
                 bidirectional=False, cell_type='lstm'):
        super().__init__()
        if cell_type == 'rnn':
            rnn = nn.RNN
        elif cell_type == 'gru':
            rnn = nn.GRU
        else:
            rnn = nn.LSTM
            
        self.rnn = rnn(input_size, hidden_size, num_layers,
                       bidirectional=bidirectional, batch_first=True, dropout=0.3)
        multiplier = 2 if bidirectional else 1
        self.fc = nn.Linear(hidden_size * multiplier, 1)
        self.dropout = nn.Dropout(0.3)
    
    def forward(self, x):
        x = x.unsqueeze(1)  # (batch, seq_len=1, features)
        out, _ = self.rnn(x)
        out = self.dropout(out[:, -1, :])
        return self.fc(out).squeeze()

# -----------------------------
# Training function
# -----------------------------
def train_model(model, epochs=15):
    model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            preds = model(batch_x)
            loss = criterion(preds, batch_y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        if (epoch+1) % 5 == 0:
            print(f"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f}")

# -----------------------------
# Evaluation
# -----------------------------
def evaluate(model):
    model.eval()
    preds, actuals = [], []
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            preds.extend(model(batch_x).cpu().numpy())
            actuals.extend(batch_y.numpy())
    
    mse = mean_squared_error(actuals, preds)
    mae = mean_absolute_error(actuals, preds)
    r2  = r2_score(actuals, preds)
    return mse, mae, r2, preds, actuals

# -----------------------------
# Train & Compare all models
# -----------------------------
results = {}
input_size = X_train.shape[1]

configs = [
    ("RNN",        "rnn",  False),
    ("Bi-RNN",     "rnn",  True),
    ("GRU",        "gru",  False),
    ("LSTM",       "lstm", False),
]

for name, cell, bi in configs:
    print(f"\n=== Training {name} ===")
    model = RecurrentRegressor(input_size, bidirectional=bi, cell_type=cell)
    train_model(model, epochs=15)
    mse, mae, r2, _, _ = evaluate(model)
    results[name] = {"MSE": round(mse, 3), "MAE": round(mae, 3), "R²": round(r2, 3)}
    print(f"{name} → MSE: {mse:.3f} | MAE: {mae:.3f} | R²: {r2:.3f}")

print("\nFinal Results:")
for k, v in results.items():
    print(f"{k}: {v}")
